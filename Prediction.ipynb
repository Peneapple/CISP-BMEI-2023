{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5392502b",
   "metadata": {},
   "source": [
    "### Define Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a364039",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/xpgeng/.conda/envs/pytorch/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import scipy\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import auc, roc_curve, matthews_corrcoef,accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score, hamming_loss, multilabel_confusion_matrix,confusion_matrix\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from torch.utils.data import TensorDataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import configparser\n",
    "from accelerate import Accelerator, DeepSpeedPlugin\n",
    "from transformers import AdamW, set_seed\n",
    "#from transformers import bioinfo_compute_metrics as compute_metrics\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "\n",
    "\n",
    "'''模型'''\n",
    "class Simple_Head(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.num_labels = config.num_labels\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.cls_dropout_prob = config.dropout\n",
    "        self.feature_size = config.feature_size\n",
    "        \n",
    "        self.in_proj = torch.nn.Linear(self.feature_size, self.hidden_size[0])\n",
    "        self.hidden_1 = torch.nn.Linear(self.hidden_size[0] , self.hidden_size[1])\n",
    "        self.hidden_2 = torch.nn.Linear(self.hidden_size[1] , self.hidden_size[2])\n",
    "        #self.hidden_3 = torch.nn.Linear(self.hidden_size[2] , self.hidden_size[3])\n",
    "        self.out_proj = torch.nn.Linear(self.hidden_size[2], self.num_labels)\n",
    "        self.batchnorm0 = nn.BatchNorm1d(self.hidden_size[0])\n",
    "        #self.batchnorm1 = nn.BatchNorm1d(self.hidden_size[1])\n",
    "        #self.batchnorm2 = nn.BatchNorm1d(self.hidden_size[2])\n",
    "        #self.batchnorm3 = nn.BatchNorm1d(self.hidden_size[3])\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(self.cls_dropout_prob) \n",
    "    \n",
    "    def forward(self, features, **kwargs):\n",
    "        x = features\n",
    "        \n",
    "        # x = [batch size, feature_dim * number_base_model]\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.view(batch_size, -1)\n",
    "        \n",
    "        x = self.dropout(F.relu(self.batchnorm0(self.in_proj(x))))\n",
    "        x = self.dropout(F.relu(self.hidden_1(x)))\n",
    "        x = self.dropout(F.relu(self.hidden_2(x)))\n",
    "        #x = self.dropout(F.relu(self.batchnorm1(self.hidden_1(x))))\n",
    "        #x = self.dropout(F.relu(self.batchnorm2(self.hidden_2(x))))\n",
    "        #x = self.dropout(F.relu(self.batchnorm3(self.hidden_3(x))))\n",
    "        x = self.out_proj(x)\n",
    "        \n",
    "        return x \n",
    "    \n",
    "class MLP_learner(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model_type = 'MLP_learner'\n",
    "        self.dropout = config.dropout\n",
    "        self.num_labels = config.num_labels\n",
    "        self.classifier = Simple_Head(config)\n",
    "        self.criterion = CrossEntropyLoss\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        \"\"\" Initialize the weights \"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "    \n",
    "    def forward(self, feature=None, \n",
    "                labels=None, weights=None):\n",
    "    \n",
    "        logits = self.classifier(feature)\n",
    "        # print(logits.shape, labels.shape)\n",
    "        loss_fct = self.criterion()\n",
    "        loss = loss_fct(logits, labels)\n",
    "\n",
    "        return_dict = {}\n",
    "        return_dict['logits'] = logits\n",
    "        return_dict['loss'] = loss\n",
    "        \n",
    "        return return_dict\n",
    "\n",
    "def simple_accuracy(preds, labels):\n",
    "    return (preds == labels).mean()\n",
    "\n",
    "def metrics(preds, labels, probs):\n",
    "    acc = simple_accuracy(preds, labels)\n",
    "\n",
    "    precision_mi = precision_score(y_true=labels, y_pred=preds, average='micro')\n",
    "    recall_mi = recall_score(y_true=labels, y_pred=preds, average='micro')\n",
    "    precision_ma = precision_score(y_true=labels, y_pred=preds, average='macro')\n",
    "    recall_ma = recall_score(y_true=labels, y_pred=preds, average='macro')\n",
    "\n",
    "    f1_macro = f1_score(y_true=labels, y_pred=preds, average='macro')\n",
    "    f1_micro = f1_score(y_true=labels, y_pred=preds, average='micro')\n",
    "\n",
    "    hamming = hamming_loss(y_true=labels, y_pred=preds)\n",
    "    auc_micro = roc_auc_score(y_true = labels, y_score = probs, average='macro', multi_class = 'ovo')\n",
    "    auc_macro = roc_auc_score(y_true = labels, y_score = probs, average='macro', multi_class = 'ovo')\n",
    "    \n",
    "    # aupr_micro = average_precision_score(y_true = labels, y_score = probs, average='macro')\n",
    "    # aupr_macro = average_precision_score(y_true = labels, y_score = probs, average='macro')\n",
    "    # auc = []\n",
    "    # for i in range(max(labels)+1):\n",
    "    #     try:\n",
    "    #         auc.append( round(roc_auc_score(y_true = labels[:,i], y_score = probs[:,i] ), 3))\n",
    "    #     except ValueError:\n",
    "    #         auc.append(0)\n",
    "    # cm = multilabel_confusion_matrix(y_true=labels, y_pred=preds)\n",
    "    return {\n",
    "        \"acc\": acc,\n",
    "        \"f1_micro\": f1_micro,\n",
    "        \"f1_macro\": f1_macro,\n",
    "        \"hamming_loss\":hamming,\n",
    "        \"precision_mi\": precision_mi,\n",
    "        \"recall_mi\": recall_mi,\n",
    "        \"precision_ma\": precision_ma,\n",
    "        \"recall_ma\": recall_ma,\n",
    "        # \"auc\": auc,\n",
    "        \"auc_micro\":auc_micro,\n",
    "        \"auc_macro\":auc_macro,\n",
    "        # \"aupr_micro\":aupr_micro,\n",
    "        # \"aupr_macro\":aupr_macro,\n",
    "        # \"confusion_matrix\": cm,\n",
    "    }\n",
    "\n",
    "def binar_metrics(preds, labels, probs):\n",
    "    acc = simple_accuracy(preds, labels)\n",
    "    precision = precision_score(y_true=labels, y_pred=preds)\n",
    "    recall = recall_score(y_true=labels, y_pred=preds)\n",
    "    f1 = f1_score(y_true=labels, y_pred=preds)\n",
    "    mcc = matthews_corrcoef(labels, preds)\n",
    "    auc = roc_auc_score(labels, probs)\n",
    "    aupr = average_precision_score(labels, probs)\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    return {\n",
    "        \"acc\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"mcc\": mcc,\n",
    "        \"auc\": auc,\n",
    "        \"aupr\": aupr,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"cm\": cm,\n",
    "    }\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "def train(model, train_dataloader, optimizer, accelerator):\n",
    "\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        \n",
    "        feature=batch[0]\n",
    "        labels=batch[1]\n",
    "        outputs = model(feature, labels)\n",
    "        loss = outputs['loss']\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "\n",
    "    return epoch_loss/len(train_dataloader)\n",
    "\n",
    "\n",
    "def evaluate(model, eval_dataloader):\n",
    "    softmax = torch.nn.Softmax(dim=1)\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    logits = None\n",
    "    \n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            feature=batch[0]\n",
    "            labels=batch[1]\n",
    "            outputs = model(feature, labels)\n",
    "        \n",
    "        if logits is None:\n",
    "            logits = outputs['logits'].detach().cpu().numpy()\n",
    "            reference = labels.detach().cpu().numpy()\n",
    "        else:\n",
    "            logits = np.append(logits,outputs['logits'].detach().cpu().numpy(), axis=0)\n",
    "            reference = np.append(reference,labels.detach().cpu().numpy(), axis=0)\n",
    "        \n",
    "        \n",
    "        loss = outputs['loss']\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    eval_loss = epoch_loss/len(eval_dataloader)\n",
    "    \n",
    "    if config.output_mode == \"boolean_cls\":\n",
    "        probs = softmax(torch.tensor(logits, dtype=torch.float32))[:,1].numpy()\n",
    "        preds = np.argmax(logits, axis=1)\n",
    "        results = binar_metrics(preds, reference, probs)\n",
    "        \n",
    "    elif config.output_mode == \"multi_cls\":\n",
    "        probs = softmax(torch.tensor(logits, dtype=torch.float32)).numpy()\n",
    "        preds = np.argmax(logits, axis=1)\n",
    "        results = metrics(preds, reference, probs)\n",
    "        \n",
    "    return eval_loss, preds, results, reference, probs\n",
    "\n",
    "\n",
    "def run_MLP(X_train, X_test, y_train, y_test, filename):\n",
    "    \n",
    "    LABELS = list(set(y_train))\n",
    "    X_train, X_test = torch.Tensor(X_train), torch.Tensor(X_test)\n",
    "    y_train, y_test = torch.LongTensor(y_train), torch.LongTensor(y_test)\n",
    "        \n",
    "    '''Configs'''\n",
    "    config.num_labels = len(LABELS)\n",
    "    config.feature_size = X_train.shape[1]\n",
    "    config.hidden_size = [128, 128, 32]\n",
    "    config.dropout = 0.1\n",
    "    config.BS = 256\n",
    "    config.learning_rate = 1e-4\n",
    "    config.weight_decay = 1e-2\n",
    "    config.num_train_epochs = 100\n",
    "    config.print_every_epoch = 10\n",
    "\n",
    "    config.output_mode = \"boolean_cls\"\n",
    "\n",
    "    # Dataset                  \n",
    "    train_set = TensorDataset(X_train, y_train)\n",
    "    val_set = TensorDataset(X_test, y_test)\n",
    "\n",
    "    # 加权采样\n",
    "    class_count = pd.DataFrame(y_train).value_counts().sort_index().to_list()\n",
    "    class_weights = 1./torch.tensor(class_count, dtype=torch.float) \n",
    "    if config.feature_size > 2000:\n",
    "        class_weights *= 1e4\n",
    "    print(\"class_weights:\", class_weights)\n",
    "\n",
    "    weighted_sampler = WeightedRandomSampler(\n",
    "        weights=class_weights[y_train],\n",
    "        num_samples=X_train.shape[0],\n",
    "        replacement=True\n",
    "    )\n",
    "\n",
    "    # Dataloader\n",
    "    train_dataloader = DataLoader(train_set, batch_size=config.BS, sampler=weighted_sampler)\n",
    "    eval_dataloader = DataLoader(val_set, batch_size=config.BS)\n",
    "\n",
    "\n",
    "    '''model'''\n",
    "    model = MLP_learner(config)\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"] \n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": config.weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "        ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=config.learning_rate)\n",
    "\n",
    "    accelerator = Accelerator() #(fp16 = True)\n",
    "\n",
    "\n",
    "    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "                model, optimizer, train_dataloader, eval_dataloader\n",
    "            )\n",
    "    \n",
    "    best_valid_loss = float('inf')\n",
    "    best_epoch  = 0 \n",
    "    for epoch in range(config.num_train_epochs):\n",
    "\n",
    "        start_time = time.monotonic()\n",
    "\n",
    "        train_loss = train(model, train_dataloader, optimizer, accelerator)\n",
    "        eval_loss, y_pred, results, _, _ = evaluate(model, eval_dataloader)\n",
    "\n",
    "        auc = results['auc']\n",
    "        rec = results['recall']\n",
    "        prec = results['precision']\n",
    "        cm = results['cm']\n",
    "        if eval_loss < best_valid_loss:\n",
    "            best_valid_loss = eval_loss\n",
    "            best_epoch = epoch\n",
    "            torch.save(model.state_dict(), f'{filename}.pt')\n",
    "\n",
    "        end_time = time.monotonic()\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        \"\"\"\n",
    "        if epoch % config.print_every_epoch == 0:\n",
    "            print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "            print(f'\\tTrain Loss: {train_loss:.3f} ')\n",
    "            print(f'\\t Val. Loss: {eval_loss:.3f} ')\n",
    "            print(f'\\t Val. Auc: {auc:.4f} | Rec: {rec:.4f} | Val. Prec: {prec:.4f}')\n",
    "            print(cm)\n",
    "        \"\"\"\n",
    "\n",
    "    \n",
    "    model.load_state_dict(torch.load(f'{filename}.pt'))\n",
    "    \n",
    "    p = time.time()\n",
    "    model.eval()\n",
    "    eval_loss, y_pred, results, y_reference, y_probs = evaluate(model, eval_dataloader)\n",
    "    q = time.time()\n",
    "    print(q-p, 'time/s')\n",
    "\n",
    "    auc = results['auc']\n",
    "    rec = results['recall']\n",
    "    prec = results['precision']\n",
    "    cm =  results['cm']\n",
    "    \n",
    "    with open('one_etoh_cross.csv', mode='a+', newline='') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        csv_writer.writerow([filename, filename, auc, rec, prec])\n",
    "        #(f'\\t Val. Auc: {auc:.4f} | Rec: {rec:.4f} | Val. Prec: {prec:.4f}')\n",
    "    print(cm)\n",
    "\n",
    "    print(sklearn.metrics.classification_report(y_pred, y_reference))\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_reference, y_probs, pos_label=1)\n",
    "    roc_auc = sklearn.metrics.auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure()\n",
    "    lw=3\n",
    "    plt.plot(fpr, tpr, label=f\"AUC: {auc:.4f}\") \n",
    "    plt.plot([0,1], [0,1], \"k--\")\n",
    "    plt.title('cross-species-etoh:'+ filename +'--->'+filename)\n",
    "    plt.xlabel(\"False positive rate\")\n",
    "    plt.ylabel(\"True positive rate\")\n",
    "    plt.legend()\n",
    "    plt.savefig('cross-species-etoh:'+ filename +'--->'+filename, dpi=1200)\n",
    "    plt.show()\n",
    "\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e534ae23",
   "metadata": {},
   "source": [
    "### Create Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bce9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build single gene name to feature dic\n",
    "import pickle\n",
    "import csv\n",
    "\n",
    "with open ('/data1/xpgeng/P1/raw_data/3_model_features_10/'+ 'iML1515' + '_10_features.pkl', 'rb') as f:\n",
    "    dicA = pickle.load(f)\n",
    "    print(len(dicA))\n",
    "\n",
    "with open ('/data1/xpgeng/P1/raw_data/4_model_features_bioembedding/'+ 'iML1515' + '_protein_embedding.pkl', 'rb') as f:\n",
    "    dicB = pickle.load(f)\n",
    "    print(len(dicB))\n",
    "\n",
    "dic={}\n",
    "for key in dicA.keys() & dicB.keys(): # use set intersection to get common keys\n",
    "  # Concatenate the values of the common keys\n",
    "    dic[key] = dicA[key] + dicB[key]\n",
    "\n",
    "# Print the result\n",
    "print(len(dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d9328d",
   "metadata": {},
   "outputs": [],
   "source": [
    "g1513= []\n",
    "for key in dic.keys():\n",
    "    g1513.append(key)\n",
    "print(len(g1513))\n",
    "\n",
    "import cobra\n",
    "model = cobra.io.read_sbml_model('/data1/xpgeng/P1/model/'+ 'iML1515.xml')\n",
    "gene = []\n",
    "for i in model.genes:\n",
    "    gene.append(i.id)\n",
    "print(len(gene))\n",
    "\n",
    "set(gene)-set(g1513)\n",
    "# {'b2092', 'b4104', 's0001'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a384805",
   "metadata": {},
   "outputs": [],
   "source": [
    "etoh ={}\n",
    "with open ('/data1/xpgeng/P1/raw_data/6_two_gene_ko/'+ 'iML1515' + '.xml_two_gene_ko', 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        if 'fail' not in row:\n",
    "            etoh[row[0]]=row[1:3]  \n",
    "            \n",
    "\n",
    "with open('iML1515'+'_two_etoh_dataset.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for key,value in etoh.items():\n",
    "        writer.writerow([*value])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0b506e",
   "metadata": {},
   "source": [
    "Here we can get all gene pair and their features. label then by the o or 1 and input MLP \\\n",
    "Noted that here I first write all the features as a file whose size is 29Gb so i dont upload it here. \\\n",
    "Next cell code is the method above, if u wanna perform the project, i suggest u choose another way. \\\n",
    "But, cause the entries is too much about 1M, so if u write them all into the MEM, i guess ur server will overload. \\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7686a2",
   "metadata": {},
   "source": [
    "### Read data and Run MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9531bd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is 29Gb and took too much time\n",
    "data = pd.read_csv('/data1/xpgeng/8_two_gene_ko_dataset/iML1515' + '_two_etoh_dataset.csv', header=None)\n",
    "p = data.iloc[:, 2068] # target labels\n",
    "n = 0\n",
    "for j in p:\n",
    "    if j == 1:\n",
    "        n +=1\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2724f128",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9d6161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this way perform prediction and splict the data into 5 parts and perform 5-fold validation\n",
    "%%time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#data = ... # load your data here\n",
    "\n",
    "X = data.iloc[:, 0:2068].values # input features\n",
    "y = data.iloc[:, 2068].values # target labels\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # split data into training and test sets\n",
    "\n",
    "\n",
    "run_MLP(X_train, X_test, y_train, y_test, 'iML1515')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34638582",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25e4996",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-pytorch] *",
   "language": "python",
   "name": "conda-env-.conda-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
